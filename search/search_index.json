{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"README.md I am excited by constantly learning of many things (Computer Science, Math, life, algorithms) Software Engineer I make common solution for game. I make MSA server framework for massive online game. I applied data analysis to balance game. How to work Set the goal. Abstract problems. Break down to sub problems. Do same thing until the remaining subproblem becomes trivial to solve. Machine Learning & Math I\u2019ve been taking ML & Math courses from MOOC 5 years ago. I have 7 machine learning certification from Coursera. I study reinforcement learning. I love Linear algebra, Partial differential equation, Probability. I am I love ideas that make invisible visible. I love a shift from doing to understanding. I am a father of world best cute girl and boy. I have sense of humour. More I love programing and enjoy it. I like vim. I use Pomodoro as a time time management method so sometimes I respond after 25 mins later. Linkedin Profile Email: resoliwan@gmail.com","title":"README.md"},{"location":"#readmemd","text":"I am excited by constantly learning of many things (Computer Science, Math, life, algorithms)","title":"README.md"},{"location":"#software-engineer","text":"I make common solution for game. I make MSA server framework for massive online game. I applied data analysis to balance game.","title":"Software Engineer"},{"location":"#how-to-work","text":"Set the goal. Abstract problems. Break down to sub problems. Do same thing until the remaining subproblem becomes trivial to solve.","title":"How to work"},{"location":"#machine-learning-math","text":"I\u2019ve been taking ML & Math courses from MOOC 5 years ago. I have 7 machine learning certification from Coursera. I study reinforcement learning. I love Linear algebra, Partial differential equation, Probability.","title":"Machine Learning &amp; Math"},{"location":"#i-am","text":"I love ideas that make invisible visible. I love a shift from doing to understanding. I am a father of world best cute girl and boy. I have sense of humour.","title":"I am"},{"location":"#more","text":"I love programing and enjoy it. I like vim. I use Pomodoro as a time time management method so sometimes I respond after 25 mins later. Linkedin Profile Email: resoliwan@gmail.com","title":"More"},{"location":"ML/MNIST/","text":"MNIST Goal Predict category of images. Data X: digit image of 28 * 28 pixels. 1-d 784 pixel vector. y: the value of image (0~9) label,pixel0,...,pixel783 1 , 0,...,0 Plan Load data make X to (784, m) make y to (10, m) using one hot encoder. run session and check feed Create model define forward model. define optimize model. define loss function. Train data. optimize loss function. Measure performance Model Linear regression Forward Model \\( f(x) = W \\cdot X + b \\) - X: (784, m), W (1, 784), b (1, 1) Loss \\( H_{q}(p) = \\sum_{x} q(x) *\\log_{2} \\frac{1}{\\log p(x)} \\) - Cross-Entroy: Average length of message from q(x) using code for p(x) Cost \\( cost(\\theta) = \\sum_{i=0}^{M}loss(x^{(i)})\\) Resource Information theory stanford-tensorflow-tutorials","title":"MNIST"},{"location":"ML/MNIST/#mnist","text":"","title":"MNIST"},{"location":"ML/MNIST/#goal","text":"Predict category of images.","title":"Goal"},{"location":"ML/MNIST/#data","text":"X: digit image of 28 * 28 pixels. 1-d 784 pixel vector. y: the value of image (0~9) label,pixel0,...,pixel783 1 , 0,...,0","title":"Data"},{"location":"ML/MNIST/#plan","text":"Load data make X to (784, m) make y to (10, m) using one hot encoder. run session and check feed Create model define forward model. define optimize model. define loss function. Train data. optimize loss function. Measure performance","title":"Plan"},{"location":"ML/MNIST/#model","text":"","title":"Model"},{"location":"ML/MNIST/#linear-regression","text":"","title":"Linear regression"},{"location":"ML/MNIST/#forward-model","text":"\\( f(x) = W \\cdot X + b \\) - X: (784, m), W (1, 784), b (1, 1)","title":"Forward Model"},{"location":"ML/MNIST/#loss","text":"\\( H_{q}(p) = \\sum_{x} q(x) *\\log_{2} \\frac{1}{\\log p(x)} \\) - Cross-Entroy: Average length of message from q(x) using code for p(x)","title":"Loss"},{"location":"ML/MNIST/#cost","text":"\\( cost(\\theta) = \\sum_{i=0}^{M}loss(x^{(i)})\\)","title":"Cost"},{"location":"ML/MNIST/#resource","text":"Information theory stanford-tensorflow-tutorials","title":"Resource"},{"location":"ML/huber_loss/","text":"Huber loss Goal Want less sensitive function to the outliers than the square error loss. How to Use square function for small value. Use absolute function for large value. IDEA \\( f(a) = \\begin{cases} a^2 & \\text{for }|a| \\leq 1, \\\\ |a| & \\text{ otherwise} \\end{cases} \\) Huber loss \\( L_{\\delta}(a) = \\begin{cases} \\frac{1}{2}a^2 & \\text{for }|a| \\leq \\delta, \\\\ \\delta (|a| - \\frac{1}{2} \\delta), & \\text{ otherwise} \\end{cases} \\) Graph - Huber loss is green (\\( \\delta = 1 \\)) - squared error loss is blue Code import tensorflow as tf import pandas as pd import numpy as np import math def huber_loss(y, y_hat, delta): diff = math.abs(y - y_hat) if diff < delta: return 0.5 * diff**2 else: return delta * diff - 0.5 * delta**2 def huber_loss(y, y_hat, delta=1.0): residual = tf.abs(y - y_hat) def square_f(): return 0.5 * tf.square(residual) def abs_f(): return delta * residual - 0.5 * tf.square(delta) return tf.cond(residual < delta, square_f, abs_f) Tag loss function ml huber loss","title":"Huber loss"},{"location":"ML/huber_loss/#huber-loss","text":"","title":"Huber loss"},{"location":"ML/huber_loss/#goal","text":"Want less sensitive function to the outliers than the square error loss.","title":"Goal"},{"location":"ML/huber_loss/#how-to","text":"Use square function for small value. Use absolute function for large value.","title":"How to"},{"location":"ML/huber_loss/#idea","text":"\\( f(a) = \\begin{cases} a^2 & \\text{for }|a| \\leq 1, \\\\ |a| & \\text{ otherwise} \\end{cases} \\)","title":"IDEA"},{"location":"ML/huber_loss/#huber-loss_1","text":"\\( L_{\\delta}(a) = \\begin{cases} \\frac{1}{2}a^2 & \\text{for }|a| \\leq \\delta, \\\\ \\delta (|a| - \\frac{1}{2} \\delta), & \\text{ otherwise} \\end{cases} \\)","title":"Huber loss"},{"location":"ML/huber_loss/#graph","text":"- Huber loss is green (\\( \\delta = 1 \\)) - squared error loss is blue","title":"Graph"},{"location":"ML/huber_loss/#code","text":"import tensorflow as tf import pandas as pd import numpy as np import math def huber_loss(y, y_hat, delta): diff = math.abs(y - y_hat) if diff < delta: return 0.5 * diff**2 else: return delta * diff - 0.5 * delta**2 def huber_loss(y, y_hat, delta=1.0): residual = tf.abs(y - y_hat) def square_f(): return 0.5 * tf.square(residual) def abs_f(): return delta * residual - 0.5 * tf.square(delta) return tf.cond(residual < delta, square_f, abs_f)","title":"Code"},{"location":"ML/huber_loss/#tag","text":"loss function ml huber loss","title":"Tag"},{"location":"ML/linear_regression/","text":"Linear regression Goal Make linear model to predict y value. Data x: birth_rate y: life_expectancy M: 190 Country Birth_rate Life_expectancy Vietnam 1.822 74.828243902 Vanuatu 3.869 70.819487805 Tonga 3.911 72.150658537 Plan Load data define input and target. Create model define forward model define optimize model define loss function. Train data Optimize loss function. Measure performance. predict data. Get measure cost. Model Linear regression. \\( f(x) = w * X + b\\) \\( loss = (y - \\hat y)^2\\) \\( cost(\\theta) = \\sum_{i=0}^{M}loss(x^{(i)})\\) Code Use Place holder. import pandas as pd import numpy as np import tensorflow as tf import matplotlib.pyplot as plt df = pd.read_csv('./examples/data/birth_life_2010.txt', delimiter='\\t') df.describe() df.shape[0] input_label = 'Birthrate' target_label = 'Lifeexpectancy' X = tf.placeholder(tf.float32, name='X') y = tf.placeholder(tf.float32, name='y') W = tf.get_variable('w', initializer=tf.constant(0.0)) b = tf.get_variable('b', initializer=tf.constant(0.0)) init_variables = tf.global_variables_initializer() y_hat = tf.multiply(W, X) + b loss = tf.sqrt((y - y_hat)**2) optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss) with tf.Session() as sess: sess.run(init_variables) writer = tf.summary.FileWriter('./graphs/linear_reg2', sess.graph) for batch in range(100): cost = 0 for i in range(df.shape[0]): _, loss_out = sess.run([optimizer, loss], feed_dict={X: df[input_label][i], y: df[target_label][i]}) cost += loss_out W_out, b_out = sess.run([W, b]) print(W_out, b_out, cost) writer.close() plt.scatter(df[input_label], df[target_label]) x_min = df[input_label].min() x_max = df[input_label].max() plt.plot([x_min, x_max], [W_out * x_min + b_out, W_out * x_max + b_out]) plt.show() Use dataset import pandas as pd import numpy as np import tensorflow as tf import matplotlib.pyplot as plt df = pd.read_csv('./examples/data/birth_life_2010.txt', delimiter='\\t') df.describe() dataset = tf.data.Dataset.from_tensor_slices((df['Birthrate'], df['Lifeexpectancy'])) iterator = dataset.make_initializable_iterator() X, y = iterator.get_next() # with tf.Session() as sess: # sess.run(iterator.initializer) # sess.run(iterator.get_next()) W = tf.get_variable('weights', initializer=tf.constant(0.0, dtype=tf.float64)) b = tf.get_variable('bias', initializer=tf.constant(0.0, dtype=tf.float64)) y_hat = W * X + b loss = tf.square(y - y_hat) optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(100): sess.run(iterator.initializer) cost = 0 try: while True: _, out_loss = sess.run([optimizer, loss]) cost += out_loss except: pass out_W, out_b = sess.run([W, b]) print('epoch %d out_W %f out_b %f cost %f' % (epoch, out_W, out_b, cost)) Resource stanford-tensorflow-tutorials","title":"Linear regression"},{"location":"ML/linear_regression/#linear-regression","text":"","title":"Linear regression"},{"location":"ML/linear_regression/#goal","text":"Make linear model to predict y value.","title":"Goal"},{"location":"ML/linear_regression/#data","text":"x: birth_rate y: life_expectancy M: 190 Country Birth_rate Life_expectancy Vietnam 1.822 74.828243902 Vanuatu 3.869 70.819487805 Tonga 3.911 72.150658537","title":"Data"},{"location":"ML/linear_regression/#plan","text":"Load data define input and target. Create model define forward model define optimize model define loss function. Train data Optimize loss function. Measure performance. predict data. Get measure cost.","title":"Plan"},{"location":"ML/linear_regression/#model","text":"Linear regression. \\( f(x) = w * X + b\\) \\( loss = (y - \\hat y)^2\\) \\( cost(\\theta) = \\sum_{i=0}^{M}loss(x^{(i)})\\)","title":"Model"},{"location":"ML/linear_regression/#code","text":"","title":"Code"},{"location":"ML/linear_regression/#use-place-holder","text":"import pandas as pd import numpy as np import tensorflow as tf import matplotlib.pyplot as plt df = pd.read_csv('./examples/data/birth_life_2010.txt', delimiter='\\t') df.describe() df.shape[0] input_label = 'Birthrate' target_label = 'Lifeexpectancy' X = tf.placeholder(tf.float32, name='X') y = tf.placeholder(tf.float32, name='y') W = tf.get_variable('w', initializer=tf.constant(0.0)) b = tf.get_variable('b', initializer=tf.constant(0.0)) init_variables = tf.global_variables_initializer() y_hat = tf.multiply(W, X) + b loss = tf.sqrt((y - y_hat)**2) optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(loss) with tf.Session() as sess: sess.run(init_variables) writer = tf.summary.FileWriter('./graphs/linear_reg2', sess.graph) for batch in range(100): cost = 0 for i in range(df.shape[0]): _, loss_out = sess.run([optimizer, loss], feed_dict={X: df[input_label][i], y: df[target_label][i]}) cost += loss_out W_out, b_out = sess.run([W, b]) print(W_out, b_out, cost) writer.close() plt.scatter(df[input_label], df[target_label]) x_min = df[input_label].min() x_max = df[input_label].max() plt.plot([x_min, x_max], [W_out * x_min + b_out, W_out * x_max + b_out]) plt.show()","title":"Use Place holder."},{"location":"ML/linear_regression/#use-dataset","text":"import pandas as pd import numpy as np import tensorflow as tf import matplotlib.pyplot as plt df = pd.read_csv('./examples/data/birth_life_2010.txt', delimiter='\\t') df.describe() dataset = tf.data.Dataset.from_tensor_slices((df['Birthrate'], df['Lifeexpectancy'])) iterator = dataset.make_initializable_iterator() X, y = iterator.get_next() # with tf.Session() as sess: # sess.run(iterator.initializer) # sess.run(iterator.get_next()) W = tf.get_variable('weights', initializer=tf.constant(0.0, dtype=tf.float64)) b = tf.get_variable('bias', initializer=tf.constant(0.0, dtype=tf.float64)) y_hat = W * X + b loss = tf.square(y - y_hat) optimizer = tf.train.GradientDescentOptimizer(learning_rate=1e-3).minimize(loss) with tf.Session() as sess: sess.run(tf.global_variables_initializer()) for epoch in range(100): sess.run(iterator.initializer) cost = 0 try: while True: _, out_loss = sess.run([optimizer, loss]) cost += out_loss except: pass out_W, out_b = sess.run([W, b]) print('epoch %d out_W %f out_b %f cost %f' % (epoch, out_W, out_b, cost))","title":"Use dataset"},{"location":"ML/linear_regression/#resource","text":"stanford-tensorflow-tutorials","title":"Resource"},{"location":"ML/softmax/","text":"Softmax Goals Input \uac12\ub4e4\uc744 \ub2e4 \ud569\uce58\uba74 1\uc774 \ub418\ub294 \ud655\ub960\uac12\uc73c\ub85c \ubcc0\ud658\ud574 \uc8fc\uace0 \uc2f6\ub2e4. \uadf8 \uc774\uc57c\uae30\ub294 \uc5b4\ub5a4 \uac12\uc774\ub358 0 \uacfc 1 \uc0ac\uc774\uc758 \uac12\uc744 \ub9ac\ud134\ud558\ub294 \ud568\uc218\ub97c \ub9cc\ub4e4\uace0 \uc2f6\ub2e4. \\( \\sigma: \\mathbb {R} ^{K}\\to (0,1)^{K} \\) Idea \uc608\ub97c\ub4e4\uc5b4 input \uac12\uc774 [1,2,3] \uc77c \uacbd\uc6b0 \uacb0\uacfc\uc758 \ucd1d\ud569\uc774 1\uc774 \ub418\uac8c \ud574\uc904\ub824\uba74 input\uc744 \ub2e4 \ub354\ud55c\uac12 6 = 1 + 2 + 3\uc73c\ub85c \ubaa8\ub4e0 \uc778\ud48b\uac12\uc744 \ub098\ub204\uc5b4 \uc8fc\uba74 \ub41c\ub2e4. \\( \\sigma([1,2,3]) \\to [\\frac{1}{6},\\frac{2}{6},\\frac{3}{6}] \\approx [0.16, 0.33, 0.5] \\approx 1 \\) \uc704\uc758 \uc2dd\uc744 \uc815\ub9ac\ud558\uba74 \uc544\ub798\uc758 \uc2dd\uc774 \ub41c\ub2e4. \\( \\sigma(\\mathbf{z_{j}}) = {\\frac {z_{j}}{\\sum_{k=1}^{K}z_{k}}}\\) \uc704\uc758 \ud568\uc218\uc758 \uc0c1\uc218 z\ub97c \uc9c0\uc218\ub85c \ubc14\uafd4\uc8fc\uc790. \\(z_j \\to e^{z_j} \\approx 2.71^{z_j}\\) \\( \\sigma(\\mathbf{z_{j}}) = {\\frac{e^{z_{j}}}{\\sum_{k=1}^{K}e^{z_{k}}}} \\) \uadf8\ub807\ub2e4 \uc704\uc758 \uc2dd\uc774 softmax \uc774\uba70 \uc6b0\ub9ac\uc758 \uc608\uc81c\ub97c \uacc4\uc0b0\ud558\uba74 \\( \\sigma([1,2,3]) \\to [\\frac{2.7}{30.2},\\frac{7.3}{30.2},\\frac{20.1}{30.2}] \\approx [0.09, 0.24, 0.66] \\approx 1 \\) Code import numpy as np x = [1,2,3] def softmax(x): np.exp(x) / np.sum(np.exp(x)) Tag softmax","title":"Softmax"},{"location":"ML/softmax/#softmax","text":"","title":"Softmax"},{"location":"ML/softmax/#goals","text":"Input \uac12\ub4e4\uc744 \ub2e4 \ud569\uce58\uba74 1\uc774 \ub418\ub294 \ud655\ub960\uac12\uc73c\ub85c \ubcc0\ud658\ud574 \uc8fc\uace0 \uc2f6\ub2e4. \uadf8 \uc774\uc57c\uae30\ub294 \uc5b4\ub5a4 \uac12\uc774\ub358 0 \uacfc 1 \uc0ac\uc774\uc758 \uac12\uc744 \ub9ac\ud134\ud558\ub294 \ud568\uc218\ub97c \ub9cc\ub4e4\uace0 \uc2f6\ub2e4. \\( \\sigma: \\mathbb {R} ^{K}\\to (0,1)^{K} \\)","title":"Goals"},{"location":"ML/softmax/#idea","text":"\uc608\ub97c\ub4e4\uc5b4 input \uac12\uc774 [1,2,3] \uc77c \uacbd\uc6b0 \uacb0\uacfc\uc758 \ucd1d\ud569\uc774 1\uc774 \ub418\uac8c \ud574\uc904\ub824\uba74 input\uc744 \ub2e4 \ub354\ud55c\uac12 6 = 1 + 2 + 3\uc73c\ub85c \ubaa8\ub4e0 \uc778\ud48b\uac12\uc744 \ub098\ub204\uc5b4 \uc8fc\uba74 \ub41c\ub2e4. \\( \\sigma([1,2,3]) \\to [\\frac{1}{6},\\frac{2}{6},\\frac{3}{6}] \\approx [0.16, 0.33, 0.5] \\approx 1 \\) \uc704\uc758 \uc2dd\uc744 \uc815\ub9ac\ud558\uba74 \uc544\ub798\uc758 \uc2dd\uc774 \ub41c\ub2e4. \\( \\sigma(\\mathbf{z_{j}}) = {\\frac {z_{j}}{\\sum_{k=1}^{K}z_{k}}}\\) \uc704\uc758 \ud568\uc218\uc758 \uc0c1\uc218 z\ub97c \uc9c0\uc218\ub85c \ubc14\uafd4\uc8fc\uc790. \\(z_j \\to e^{z_j} \\approx 2.71^{z_j}\\) \\( \\sigma(\\mathbf{z_{j}}) = {\\frac{e^{z_{j}}}{\\sum_{k=1}^{K}e^{z_{k}}}} \\) \uadf8\ub807\ub2e4 \uc704\uc758 \uc2dd\uc774 softmax \uc774\uba70 \uc6b0\ub9ac\uc758 \uc608\uc81c\ub97c \uacc4\uc0b0\ud558\uba74 \\( \\sigma([1,2,3]) \\to [\\frac{2.7}{30.2},\\frac{7.3}{30.2},\\frac{20.1}{30.2}] \\approx [0.09, 0.24, 0.66] \\approx 1 \\)","title":"Idea"},{"location":"ML/softmax/#code","text":"import numpy as np x = [1,2,3] def softmax(x): np.exp(x) / np.sum(np.exp(x))","title":"Code"},{"location":"ML/softmax/#tag","text":"softmax","title":"Tag"},{"location":"ML/tensorflow_dataset/","text":"tenserflow data Goal Provide data abstraction. tf.Data.Dataset Represent sequence of elements. Creating from source.(tf.Tensor) Creating from tf.dataset using Transformation. tf.data.Iterator Provide main way to extract data from Dataset Sample data l,d0,d1,d2,d3,d4 0,1,2,3,4 1,11,12,13,14 2,21,22,23,24 import pandas as pd import tensorflow as tf import numpy as np df = pd.read_csv('./data/test.csv') df.describe() dm = df.as_matrix() y = dm[:, 0] x = dm[:, 1:] max = np.max(dm[:, 0]) + 1 onehot_y = np.eye(max)[dm[:, 0]] dataset = tf.data.Dataset.from_tensor_slices((x, onehot_y)) iterator = dataset.make_initializable_iterator() X, y = iterator.get_next() with tf.Session() as sess: for e in range(3): sess.run(iterator.initializer) try: while True: X_out, y_out = sess.run([X, y]) print('X_out', X_out) print('y_out', y_out) except tf.errors.OutOfRangeError: print('OutOfRangeError') pass Tag tensorflow dataset iterator","title":"tenserflow data"},{"location":"ML/tensorflow_dataset/#tenserflow-data","text":"","title":"tenserflow data"},{"location":"ML/tensorflow_dataset/#goal","text":"Provide data abstraction. tf.Data.Dataset Represent sequence of elements. Creating from source.(tf.Tensor) Creating from tf.dataset using Transformation. tf.data.Iterator Provide main way to extract data from Dataset","title":"Goal"},{"location":"ML/tensorflow_dataset/#sample-data","text":"l,d0,d1,d2,d3,d4 0,1,2,3,4 1,11,12,13,14 2,21,22,23,24 import pandas as pd import tensorflow as tf import numpy as np df = pd.read_csv('./data/test.csv') df.describe() dm = df.as_matrix() y = dm[:, 0] x = dm[:, 1:] max = np.max(dm[:, 0]) + 1 onehot_y = np.eye(max)[dm[:, 0]] dataset = tf.data.Dataset.from_tensor_slices((x, onehot_y)) iterator = dataset.make_initializable_iterator() X, y = iterator.get_next() with tf.Session() as sess: for e in range(3): sess.run(iterator.initializer) try: while True: X_out, y_out = sess.run([X, y]) print('X_out', X_out) print('y_out', y_out) except tf.errors.OutOfRangeError: print('OutOfRangeError') pass","title":"Sample data"},{"location":"ML/tensorflow_dataset/#tag","text":"tensorflow dataset iterator","title":"Tag"},{"location":"algorithm/en/1_introduction/","text":"Introduction 1. The Role of Algorithm in Computing. What are algorithm? An algorithm is any well-defined procedure that takes values as input and produces some value as output. like function. The statement of problem specifies the desired input and output relationship. The algorithm is procedure for archieving input/output relationship. Ex. Sorting problem Input : A sequence of n elements \\( A = \\). Output : The permutation(reordering) \\( A = \\) of the input sequence such that satisfies \\( a_{1}^{\\prime} \\le a_{2}^{\\prime} \\le ... \\le a_{n}^{\\prime} \\). Why is the study algorithms worthwhile? To learn techniques of algorithm design and analysis. 2. Getting Started From https://mitpress.mit.edu/books/introduction-algorithms-third-edition","title":"Introduction"},{"location":"algorithm/en/1_introduction/#introduction","text":"","title":"Introduction"},{"location":"algorithm/en/1_introduction/#1-the-role-of-algorithm-in-computing","text":"What are algorithm? An algorithm is any well-defined procedure that takes values as input and produces some value as output. like function. The statement of problem specifies the desired input and output relationship. The algorithm is procedure for archieving input/output relationship. Ex. Sorting problem Input : A sequence of n elements \\( A = \\). Output : The permutation(reordering) \\( A = \\) of the input sequence such that satisfies \\( a_{1}^{\\prime} \\le a_{2}^{\\prime} \\le ... \\le a_{n}^{\\prime} \\). Why is the study algorithms worthwhile? To learn techniques of algorithm design and analysis.","title":"1. The Role of Algorithm in Computing."},{"location":"algorithm/en/1_introduction/#2-getting-started","text":"","title":"2. Getting Started"},{"location":"algorithm/en/1_introduction/#from","text":"https://mitpress.mit.edu/books/introduction-algorithms-third-edition","title":"From"},{"location":"algorithm/en/2_getting_started/","text":"Check list for learning algorithm Idea. Pseudocode. Argue that it correctly works. Analyze its running time. Problem Input : A sequence of n elements \\( A = \\). Output : The permutation(reordering) \\( A = \\) of the input sequence such that satisfies \\( a_{1}^{\\prime} \\le a_{2}^{\\prime} \\le ... \\le a_{n}^{\\prime} \\). Insertion sort Idea Idea: incremental approch. increment sorted array. insert current item into the sorted arrary A[1..j - 1] HowTo: Start from sorted length 1 array. Until conditions meet - i > 0 and A[i] > key Move i to one step back. Decrease i. Insert current item in A[i + 1] Pseduocode Persue clean and concise for j = 2 to A.length key = A[j] // Insert A[j] into the sorted sequence A[1..j - 1] i = j - 1 while i > 0 and A[i] > key A[i + 1] = A[i] i = i - 1 A[i + 1] = key Tip - Use \"i\" as a important loop variable name. - Use simple condition first when there is logical concanation. - Use left as variable and right to almost constant value when compare two numbers. - Make it simple as possible Argue that it correctly works","title":"Check list for learning algorithm"},{"location":"algorithm/en/2_getting_started/#check-list-for-learning-algorithm","text":"Idea. Pseudocode. Argue that it correctly works. Analyze its running time.","title":"Check list for learning algorithm"},{"location":"algorithm/en/2_getting_started/#problem","text":"Input : A sequence of n elements \\( A = \\). Output : The permutation(reordering) \\( A = \\) of the input sequence such that satisfies \\( a_{1}^{\\prime} \\le a_{2}^{\\prime} \\le ... \\le a_{n}^{\\prime} \\).","title":"Problem"},{"location":"algorithm/en/2_getting_started/#insertion-sort","text":"","title":"Insertion sort"},{"location":"algorithm/en/2_getting_started/#idea","text":"Idea: incremental approch. increment sorted array. insert current item into the sorted arrary A[1..j - 1] HowTo: Start from sorted length 1 array. Until conditions meet - i > 0 and A[i] > key Move i to one step back. Decrease i. Insert current item in A[i + 1]","title":"Idea"},{"location":"algorithm/en/2_getting_started/#pseduocode","text":"Persue clean and concise for j = 2 to A.length key = A[j] // Insert A[j] into the sorted sequence A[1..j - 1] i = j - 1 while i > 0 and A[i] > key A[i + 1] = A[i] i = i - 1 A[i + 1] = key Tip - Use \"i\" as a important loop variable name. - Use simple condition first when there is logical concanation. - Use left as variable and right to almost constant value when compare two numbers. - Make it simple as possible","title":"Pseduocode"},{"location":"algorithm/en/2_getting_started/#argue-that-it-correctly-works","text":"","title":"Argue that it correctly works"},{"location":"algorithm/kr/1_introduction/","text":"\ub4e4\uc5b4\uac00\uae30 \uc804\uc5d0 1. \uc54c\uace0\ub9ac\uc998\uc758 \uc5ed\ud560 \uc54c\uace0\ub9ac\uc998\uc740 \ubb34\uc5c7\uc778\uac00? \ud568\uc218\uc801 \uad00\uc810 \uc54c\uace0\ub9ac\uc998\uc740 \ud2b9\uc815 \uac12\uc744 \uc778\ud48b\uc73c\ub85c \ubc1b\uc544 \ud2b9\uc815 \uac12\uc744 \uc544\uc6c3\ud48b\uc73c\ub85c \ub3cc\ub824\uc8fc\ub294 \ud568\uc218\ub77c\uace0 \uc0dd\uac01 \ud560 \uc218 \uc788\ub2e4. \uc54c\uace0\ub9ac\uc998\uc740 \ud574\ub2f9 \ud568\uc218\ub97c \ud589\ud558\ub294\ub370 \ud544\uc694\ud55c \uba85\ub839\uc5b4\ub4e4\uc758 \ub9ac\uc2a4\ud2b8\ub77c\uace0 \ud560 \uc218 \uc788\ub2e4. \uad00\uacc4\uc801 \uad00\uc810 \uc778\ud48b \uac12\uacfc \uc544\uc6c3\ud48b \uac12\uc758 \uad00\uacc4\uac00 \uc815\uc758\ub41c \ubb38\uc81c\ub85c \ubcf4\ub294 \ubc29\ubc95\ub3c4 \uc788\ub2e4. \uc54c\uace0\ub9ac\uc998\uc740 \uc778\ud48b \uac12\uacfc \uc544\uc6c3\ud48b \uac12\uc758 \uad00\uacc4\ub97c \ub9cc\uc871\uc2dc\ud0a4\uac8c \ud558\ub294 \uc77c\ub828\uc758 \uc808\ucc28\ub77c\uace0 \ud560 \uc218 \uc788\ub2e4. Ex. \uc815\ub82c \ubb38\uc81c Input : n \uac1c\uc758 \uc694\uc18c\ub85c \uad6c\uc131\ub41c \uc21c\uc11c\uc5f4 \\( A = \\). Output : \\( a_{1}^{\\prime} \\le a_{2}^{\\prime} \\le ... \\le a_{n}^{\\prime} \\) \uc744 \ub9cc\uc871\ud558\ub294, \uc7ac \uc815\ub82c\ub41c \uc21c\uc11c\uc5f4 \\( A = \\) . \uc54c\uace0\ub9ac\uc998\uc744 \ud559\uc2b5\ud574\uc57c \ud558\ub294 \uc774\uc720. \ubb38\uc81c\ub97c \ud478\ub294 \ubc29\ubc95 \uc790\uccb4\uc5d0 \ub300\ud55c \uc124\uacc4 \ud328\ud134, \uc131\ub2a5 \uce21\uc815 \uc694\uc18c \ub4f1\uc744 \ud559\uc2b5\ud560 \uc218 \uc788\ub2e4. \ubb38\uc81c\ub97c \ud478\ub294 \uc88b\uc740 \ubc29\ubc95\uc744 \ubc30\uc6b0\uae30 \uc704\ud574\uc11c \ud559\uc2b5\ud55c\ub2e4.","title":"\ub4e4\uc5b4\uac00\uae30 \uc804\uc5d0"},{"location":"algorithm/kr/1_introduction/#_1","text":"","title":"\ub4e4\uc5b4\uac00\uae30 \uc804\uc5d0"},{"location":"algorithm/kr/1_introduction/#1","text":"","title":"1. \uc54c\uace0\ub9ac\uc998\uc758 \uc5ed\ud560"},{"location":"algorithm/kr/1_introduction/#_2","text":"\ud568\uc218\uc801 \uad00\uc810 \uc54c\uace0\ub9ac\uc998\uc740 \ud2b9\uc815 \uac12\uc744 \uc778\ud48b\uc73c\ub85c \ubc1b\uc544 \ud2b9\uc815 \uac12\uc744 \uc544\uc6c3\ud48b\uc73c\ub85c \ub3cc\ub824\uc8fc\ub294 \ud568\uc218\ub77c\uace0 \uc0dd\uac01 \ud560 \uc218 \uc788\ub2e4. \uc54c\uace0\ub9ac\uc998\uc740 \ud574\ub2f9 \ud568\uc218\ub97c \ud589\ud558\ub294\ub370 \ud544\uc694\ud55c \uba85\ub839\uc5b4\ub4e4\uc758 \ub9ac\uc2a4\ud2b8\ub77c\uace0 \ud560 \uc218 \uc788\ub2e4. \uad00\uacc4\uc801 \uad00\uc810 \uc778\ud48b \uac12\uacfc \uc544\uc6c3\ud48b \uac12\uc758 \uad00\uacc4\uac00 \uc815\uc758\ub41c \ubb38\uc81c\ub85c \ubcf4\ub294 \ubc29\ubc95\ub3c4 \uc788\ub2e4. \uc54c\uace0\ub9ac\uc998\uc740 \uc778\ud48b \uac12\uacfc \uc544\uc6c3\ud48b \uac12\uc758 \uad00\uacc4\ub97c \ub9cc\uc871\uc2dc\ud0a4\uac8c \ud558\ub294 \uc77c\ub828\uc758 \uc808\ucc28\ub77c\uace0 \ud560 \uc218 \uc788\ub2e4. Ex. \uc815\ub82c \ubb38\uc81c Input : n \uac1c\uc758 \uc694\uc18c\ub85c \uad6c\uc131\ub41c \uc21c\uc11c\uc5f4 \\( A = \\). Output : \\( a_{1}^{\\prime} \\le a_{2}^{\\prime} \\le ... \\le a_{n}^{\\prime} \\) \uc744 \ub9cc\uc871\ud558\ub294, \uc7ac \uc815\ub82c\ub41c \uc21c\uc11c\uc5f4 \\( A = \\) .","title":"\uc54c\uace0\ub9ac\uc998\uc740 \ubb34\uc5c7\uc778\uac00?"},{"location":"algorithm/kr/1_introduction/#_3","text":"\ubb38\uc81c\ub97c \ud478\ub294 \ubc29\ubc95 \uc790\uccb4\uc5d0 \ub300\ud55c \uc124\uacc4 \ud328\ud134, \uc131\ub2a5 \uce21\uc815 \uc694\uc18c \ub4f1\uc744 \ud559\uc2b5\ud560 \uc218 \uc788\ub2e4. \ubb38\uc81c\ub97c \ud478\ub294 \uc88b\uc740 \ubc29\ubc95\uc744 \ubc30\uc6b0\uae30 \uc704\ud574\uc11c \ud559\uc2b5\ud55c\ub2e4.","title":"\uc54c\uace0\ub9ac\uc998\uc744 \ud559\uc2b5\ud574\uc57c \ud558\ub294  \uc774\uc720."},{"location":"idea/launcher/","text":"Luncher App Goal \uc54c\ud504\ub808\ub4dc\uc640 \uac19\uc740 \uc548\ub4dc\ub85c\uc774\ub4dc \ub7f0\ucc98\ub97c \ub9cc\ub4e4\uc790. Why? \uc548\ub4dc\ub85c\uc774\ub4dc\uc5d0\uc11c \uc571\uc744 \ucc3e\uc744 \ub54c \uadc0\ucc2e\ub2e4. Functions \ucd08\uc131 \uac80\uc0c9\uc774 \ub41c\ub2e4. \ub7f0\uccd0\ub97c \ub9cc\ub4e4\uace0 \ub0a0\uc790, wifi, \uc704\uce58 \ub4f1\uc744 \uc0ac\uc6a9\ud574 \uc790\uc8fc \uc0ac\uc6a9\ud558\ub294 \uc571\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uba38\uc2e0\ub7ec\ub2dd\uc744 \uc0ac\uc6a9\ud574 \ub7f0\ucc98\uc5d0 \ub178\ucd9c\ub418\ub294 \uac83\ub4e4\uc744 \ud559\uc2b5\uc2dc\ud0a8\ub2e4. Formula \\( P(\\text{Prioriry} | \\text{date, wifi, lat, lng}) \\)","title":"Luncher App"},{"location":"idea/launcher/#luncher-app","text":"","title":"Luncher App"},{"location":"idea/launcher/#goal","text":"\uc54c\ud504\ub808\ub4dc\uc640 \uac19\uc740 \uc548\ub4dc\ub85c\uc774\ub4dc \ub7f0\ucc98\ub97c \ub9cc\ub4e4\uc790.","title":"Goal"},{"location":"idea/launcher/#why","text":"\uc548\ub4dc\ub85c\uc774\ub4dc\uc5d0\uc11c \uc571\uc744 \ucc3e\uc744 \ub54c \uadc0\ucc2e\ub2e4.","title":"Why?"},{"location":"idea/launcher/#functions","text":"\ucd08\uc131 \uac80\uc0c9\uc774 \ub41c\ub2e4. \ub7f0\uccd0\ub97c \ub9cc\ub4e4\uace0 \ub0a0\uc790, wifi, \uc704\uce58 \ub4f1\uc744 \uc0ac\uc6a9\ud574 \uc790\uc8fc \uc0ac\uc6a9\ud558\ub294 \uc571\uc744 \ubcf4\uc5ec\uc900\ub2e4. \uba38\uc2e0\ub7ec\ub2dd\uc744 \uc0ac\uc6a9\ud574 \ub7f0\ucc98\uc5d0 \ub178\ucd9c\ub418\ub294 \uac83\ub4e4\uc744 \ud559\uc2b5\uc2dc\ud0a8\ub2e4.","title":"Functions"},{"location":"idea/launcher/#formula","text":"\\( P(\\text{Prioriry} | \\text{date, wifi, lat, lng}) \\)","title":"Formula"},{"location":"mkdocs/usage/","text":"HowTo Add javascrtip Mathjax.js add below configuration at mkdocs extra_javascript: - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML Deploy cd home mkdocs build cd site git add . git commit -m git push open https://resoliwan.github.io","title":"HowTo"},{"location":"mkdocs/usage/#howto","text":"","title":"HowTo"},{"location":"mkdocs/usage/#add-javascrtip","text":"","title":"Add javascrtip"},{"location":"mkdocs/usage/#mathjaxjs","text":"add below configuration at mkdocs extra_javascript: - https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML","title":"Mathjax.js"},{"location":"mkdocs/usage/#deploy","text":"cd home mkdocs build cd site git add . git commit -m git push open https://resoliwan.github.io","title":"Deploy"}]}